{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "614e6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
    "documents = requests.get(docs_url).json()\n",
    "\n",
    "ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7627759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def hit_rate(relevance_total): # Define hitrate evaluation function\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea2eca",
   "metadata": {},
   "source": [
    "### Answer:1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0a90bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x183db92f250>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Minsearch index with specified text and keyword fields\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"section\", \"text\"],\n",
    "    keyword_fields=[\"course\", \"id\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99f8f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define boosting parameters as specified\n",
    "boost = {'question': 1.5, 'section': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a712644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search function that uses boosting and filters by course\n",
    "def minsearch_search(query, course):\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': course},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba3f0a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22614512bd1549c9815e80fa3d1e8e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate hitrate over all queries in ground truth\n",
    "relevance_total = []\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = minsearch_search(query=q['question'], course=q['course'])\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b412a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minsearch hitrate (with boost): 0.848714069591528\n"
     ]
    }
   ],
   "source": [
    "hit = hit_rate(relevance_total)\n",
    "print(\"Minsearch hitrate (with boost):\", hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34262326",
   "metadata": {},
   "source": [
    "### Answer:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "808b018d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a6ed75fb97476ab133174c59f93538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Vector Search, question field): 0.35673582594913916\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "import minsearch  # Your minsearch.py containing Index class\n",
    "\n",
    "# Load data\n",
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "documents = requests.get(url_prefix + 'search_evaluation/documents-with-ids.json').json()\n",
    "df_ground_truth = pd.read_csv(url_prefix + 'search_evaluation/ground-truth-data.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "# Create embeddings\n",
    "texts = [doc['question'] for doc in documents]\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)\n",
    "\n",
    "# Build index and inject embeddings\n",
    "index = minsearch.Index(text_fields=[], keyword_fields=['course', 'id'])\n",
    "index.docs = documents\n",
    "index.keyword_df = pd.DataFrame({field: [doc.get(field, '') for doc in documents] for field in index.keyword_fields})\n",
    "index.embeddings = X  # Add embeddings attribute\n",
    "\n",
    "def vector_search(idx, query_vec, filter_dict={}, num_results=5):\n",
    "    def cosine_similarity(a, b):\n",
    "        a_norm = np.linalg.norm(a)\n",
    "        b_norms = np.linalg.norm(b, axis=1)\n",
    "        return np.dot(b, a) / (b_norms * a_norm + 1e-10)\n",
    "\n",
    "    if hasattr(query_vec, 'toarray'):\n",
    "        qvec = query_vec.toarray().flatten()\n",
    "    else:\n",
    "        qvec = query_vec.flatten()\n",
    "\n",
    "    sims = cosine_similarity(qvec, idx.embeddings)\n",
    "\n",
    "    mask = np.ones(len(idx.docs), dtype=bool)\n",
    "    for field, val in filter_dict.items():\n",
    "        if field in idx.keyword_fields:\n",
    "            arr = np.array(idx.keyword_df[field])\n",
    "            mask = mask & (arr == val)\n",
    "\n",
    "    sims = sims * mask\n",
    "\n",
    "    top_indices = np.argpartition(sims, -num_results)[-num_results:]\n",
    "    top_indices = top_indices[np.argsort(-sims[top_indices])]\n",
    "\n",
    "    results = [idx.docs[i] for i in top_indices if sims[i] > 0]\n",
    "    if not results:\n",
    "        # if no documents with positive similarity, fall back to top ranked regardless of positive score\n",
    "        results = [idx.docs[i] for i in top_indices]\n",
    "    return results\n",
    "\n",
    "# Search function wrapped for evaluation\n",
    "def search_fn(q):\n",
    "    query_vec = pipeline.transform([q['question']])\n",
    "    return vector_search(index, query_vec, filter_dict={'course': q['course']}, num_results=5)\n",
    "\n",
    "# MRR evaluation\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank]:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "# Evaluate MRR\n",
    "relevance_total = []\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = search_fn(q)\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)\n",
    "\n",
    "score = mrr(relevance_total)\n",
    "print(\"MRR (Vector Search, question field):\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103626db",
   "metadata": {},
   "source": [
    "### Answer:3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb01273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538024e54d2f47eb8be826bda00b62e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitrate (Vector Search question + text): 0.8210503566025502\n"
     ]
    }
   ],
   "source": [
    "# Combine question and text for document embeddings\n",
    "texts = [doc['question'] + ' ' + doc['text'] for doc in documents]\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)\n",
    "\n",
    "# Build the index\n",
    "index = minsearch.Index(text_fields=[], keyword_fields=['course', 'id'])\n",
    "index.docs = documents\n",
    "index.keyword_df = pd.DataFrame({field: [doc.get(field, '') for doc in documents] for field in index.keyword_fields})\n",
    "index.embeddings = X\n",
    "\n",
    "def vector_search(index, query_vec, filter_dict={}, num_results=5):\n",
    "    def cosine_similarity(a, b):\n",
    "        a_norm = np.linalg.norm(a)\n",
    "        b_norms = np.linalg.norm(b, axis=1)\n",
    "        return np.dot(b, a) / (b_norms * a_norm + 1e-10)\n",
    "\n",
    "    if hasattr(query_vec, 'toarray'):\n",
    "        qvec = query_vec.toarray().flatten()\n",
    "    else:\n",
    "        qvec = query_vec.flatten()\n",
    "    sims = cosine_similarity(qvec, index.embeddings)\n",
    "\n",
    "    mask = np.ones(len(index.docs), dtype=bool)\n",
    "    for field, val in filter_dict.items():\n",
    "        if field in index.keyword_fields:\n",
    "            arr = np.array(index.keyword_df[field])\n",
    "            mask = mask & (arr == val)\n",
    "\n",
    "    sims = sims * mask\n",
    "    top_indices = np.argpartition(sims, -num_results)[-num_results:]\n",
    "    top_indices = top_indices[np.argsort(-sims[top_indices])]\n",
    "    results = [index.docs[i] for i in top_indices if sims[i] > 0]\n",
    "    if not results:\n",
    "        results = [index.docs[i] for i in top_indices]\n",
    "    return results\n",
    "\n",
    "def vector_search_question_text(q):\n",
    "    query_text = q['question']\n",
    "    course_filter = q['course']\n",
    "    query_vec = pipeline.transform([query_text])\n",
    "    return vector_search(index, query_vec, filter_dict={'course': course_filter}, num_results=5)\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    return sum(True in line for line in relevance_total) / len(relevance_total)\n",
    "\n",
    "relevance_total = []\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = vector_search_question_text(q)\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)\n",
    "\n",
    "hit = hit_rate(relevance_total)\n",
    "print(\"Hitrate (Vector Search question + text):\", hit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c755bf",
   "metadata": {},
   "source": [
    "### Answer:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6500b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tempfile\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 0. Global configuration\n",
    "# ------------------------------------------------------------------#\n",
    "QDRANT_HOST       = os.getenv(\"QDRANT_HOST\", \"http://localhost:6333\")\n",
    "COLLECTION        = \"zoomcamp-rag\"\n",
    "EMBED_MODEL       = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "VECTOR_DIM        = 512              # According to FastEmbed manifest\n",
    "TOP_K             = 5                # Evaluation limit\n",
    "TMP_CACHE_DIR     = os.path.join(tempfile.gettempdir(), \"fastembed_cache\")\n",
    "\n",
    "URL_PREFIX        = \"https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/\"\n",
    "DOCS_URL          = URL_PREFIX + \"search_evaluation/documents-with-ids.json\"\n",
    "GROUND_TRUTH_URL  = URL_PREFIX + \"search_evaluation/ground-truth-data.csv\"\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 1. Helper functions for metrics\n",
    "# ------------------------------------------------------------------#\n",
    "def hit_rate(relevance_total):\n",
    "    \"\"\"Proportion of queries where the correct doc is in the candidate list.\"\"\"\n",
    "    return sum(any(line) for line in relevance_total) / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    \"\"\"Mean Reciprocal Rank over binary relevance lists.\"\"\"\n",
    "    total = 0.0\n",
    "    for line in relevance_total:\n",
    "        for idx, is_rel in enumerate(line):\n",
    "            if is_rel:\n",
    "                total += 1.0 / (idx + 1)\n",
    "                break\n",
    "    return total / len(relevance_total)\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 2. Dataset download\n",
    "# ------------------------------------------------------------------#\n",
    "def download_dataset():\n",
    "    print(\"📥  Downloading dataset …\")\n",
    "    docs   = requests.get(DOCS_URL, timeout=30).json()\n",
    "    gtruth = pd.read_csv(GROUND_TRUTH_URL).to_dict(orient=\"records\")\n",
    "    print(f\"   → Documents      : {len(docs)}\")\n",
    "    print(f\"   → Ground-truth Q : {len(gtruth)}\")\n",
    "    return docs, gtruth\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 3. Qdrant initialization\n",
    "# ------------------------------------------------------------------#\n",
    "def init_qdrant():\n",
    "    client = QdrantClient(QDRANT_HOST)\n",
    "    if client.collection_exists(COLLECTION):\n",
    "        print(f\"🗑️  Deleting stale collection “{COLLECTION}” …\")\n",
    "        client.delete_collection(collection_name=COLLECTION)\n",
    "        # Wait a moment to avoid race conditions with re-creation\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    print(f\"🆕  Creating collection “{COLLECTION}” …\")\n",
    "    client.create_collection(\n",
    "        collection_name = COLLECTION,\n",
    "        vectors_config  = models.VectorParams(\n",
    "            size     = VECTOR_DIM,\n",
    "            distance = Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "    return client\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 4. Embedding & ingestion\n",
    "# ------------------------------------------------------------------#\n",
    "def ingest_documents(client, documents):\n",
    "    print(\"🧪  Loading embedding model (FastEmbed)…\")\n",
    "    embedder = TextEmbedding(model_name=EMBED_MODEL, cache_dir=TMP_CACHE_DIR)\n",
    "\n",
    "    # ❶ Build the full text list first\n",
    "    texts   = [f\"{d['question']} {d['text']}\" for d in documents]\n",
    "    ids     = [d[\"id\"] for d in documents]\n",
    "\n",
    "    # ❷ Generate all vectors at once (generator → list)\n",
    "    vectors = list(embedder.embed(texts))      # ← converts generator\n",
    "\n",
    "    # ❸ Package points for Qdrant\n",
    "    points  = [\n",
    "        models.PointStruct(\n",
    "            id      = pid,\n",
    "            vector  = vec,\n",
    "            payload = {\n",
    "                \"question\": documents[i][\"question\"],\n",
    "                \"text\"    : documents[i][\"text\"]\n",
    "            }\n",
    "        )\n",
    "        for i, (pid, vec) in enumerate(zip(ids, vectors))\n",
    "    ]\n",
    "\n",
    "    print(\"⏫  Upserting into Qdrant …\")\n",
    "    client.upsert(collection_name=COLLECTION, points=points)\n",
    "    print(\"✅  Ingestion complete\")\n",
    "# ------------------------------------------------------------------#\n",
    "# 5. Search wrapper\n",
    "# ------------------------------------------------------------------#\n",
    "def make_search_fn(client):\n",
    "    embedder = TextEmbedding(model_name=EMBED_MODEL, cache_dir=TMP_CACHE_DIR)\n",
    "\n",
    "    def _search(q):\n",
    "        q_vec = embedder.embed([q[\"question\"]])[0]\n",
    "        hits  = client.search(\n",
    "            collection_name = COLLECTION,\n",
    "            query_vector    = q_vec,\n",
    "            limit           = TOP_K\n",
    "        )\n",
    "        # Reshape into expected dict list\n",
    "        return [{\"id\": hit.id} for hit in hits]\n",
    "\n",
    "    return _search\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 6. Evaluation orchestrator\n",
    "# ------------------------------------------------------------------#\n",
    "def evaluate(ground_truth, search_fn):\n",
    "    relevance_total = []\n",
    "    for q in tqdm(ground_truth, desc=\"🥇  Evaluating\", unit=\"query\"):\n",
    "        correct_id  = q[\"document\"]\n",
    "        candidates  = search_fn(q)\n",
    "        line        = [cand[\"id\"] == correct_id for cand in candidates]\n",
    "        relevance_total.append(line)\n",
    "\n",
    "    return {\n",
    "        \"hit_rate\": hit_rate(relevance_total),\n",
    "        \"mrr\"     : mrr(relevance_total)\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 7. Main\n",
    "# ------------------------------------------------------------------#\n",
    "def main():\n",
    "    print(\"🚀  Starting pipeline\")\n",
    "    docs, gtruth  = download_dataset()\n",
    "    client        = init_qdrant()\n",
    "    ingest_documents(client, docs)\n",
    "    search_fn     = make_search_fn(client)\n",
    "    metrics       = evaluate(gtruth, search_fn)\n",
    "\n",
    "    print(\"\\n📊  Final metrics\")\n",
    "    print(f\"Hit Rate: {metrics['hit_rate']:.4f}\")\n",
    "    print(f\"MRR      : {metrics['mrr']:.4f}\")\n",
    "\n",
    "# Entry-point guard\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

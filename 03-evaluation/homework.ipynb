{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "614e6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "docs_url = url_prefix + 'search_evaluation/documents-with-ids.json'\n",
    "documents = requests.get(docs_url).json()\n",
    "\n",
    "ground_truth_url = url_prefix + 'search_evaluation/ground-truth-data.csv'\n",
    "df_ground_truth = pd.read_csv(ground_truth_url)\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7627759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def hit_rate(relevance_total): # Define hitrate evaluation function\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea2eca",
   "metadata": {},
   "source": [
    "### Answer:1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0a90bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x183db92f250>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Minsearch index with specified text and keyword fields\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"section\", \"text\"],\n",
    "    keyword_fields=[\"course\", \"id\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99f8f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define boosting parameters as specified\n",
    "boost = {'question': 1.5, 'section': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a712644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search function that uses boosting and filters by course\n",
    "def minsearch_search(query, course):\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': course},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba3f0a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22614512bd1549c9815e80fa3d1e8e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate hitrate over all queries in ground truth\n",
    "relevance_total = []\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = minsearch_search(query=q['question'], course=q['course'])\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b412a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minsearch hitrate (with boost): 0.848714069591528\n"
     ]
    }
   ],
   "source": [
    "hit = hit_rate(relevance_total)\n",
    "print(\"Minsearch hitrate (with boost):\", hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34262326",
   "metadata": {},
   "source": [
    "### Answer:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "808b018d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a6ed75fb97476ab133174c59f93538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR (Vector Search, question field): 0.35673582594913916\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "import minsearch  # Your minsearch.py containing Index class\n",
    "\n",
    "# Load data\n",
    "url_prefix = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/'\n",
    "documents = requests.get(url_prefix + 'search_evaluation/documents-with-ids.json').json()\n",
    "df_ground_truth = pd.read_csv(url_prefix + 'search_evaluation/ground-truth-data.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')\n",
    "\n",
    "# Create embeddings\n",
    "texts = [doc['question'] for doc in documents]\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)\n",
    "\n",
    "# Build index and inject embeddings\n",
    "index = minsearch.Index(text_fields=[], keyword_fields=['course', 'id'])\n",
    "index.docs = documents\n",
    "index.keyword_df = pd.DataFrame({field: [doc.get(field, '') for doc in documents] for field in index.keyword_fields})\n",
    "index.embeddings = X  # Add embeddings attribute\n",
    "\n",
    "def vector_search(idx, query_vec, filter_dict={}, num_results=5):\n",
    "    def cosine_similarity(a, b):\n",
    "        a_norm = np.linalg.norm(a)\n",
    "        b_norms = np.linalg.norm(b, axis=1)\n",
    "        return np.dot(b, a) / (b_norms * a_norm + 1e-10)\n",
    "\n",
    "    if hasattr(query_vec, 'toarray'):\n",
    "        qvec = query_vec.toarray().flatten()\n",
    "    else:\n",
    "        qvec = query_vec.flatten()\n",
    "\n",
    "    sims = cosine_similarity(qvec, idx.embeddings)\n",
    "\n",
    "    mask = np.ones(len(idx.docs), dtype=bool)\n",
    "    for field, val in filter_dict.items():\n",
    "        if field in idx.keyword_fields:\n",
    "            arr = np.array(idx.keyword_df[field])\n",
    "            mask = mask & (arr == val)\n",
    "\n",
    "    sims = sims * mask\n",
    "\n",
    "    top_indices = np.argpartition(sims, -num_results)[-num_results:]\n",
    "    top_indices = top_indices[np.argsort(-sims[top_indices])]\n",
    "\n",
    "    results = [idx.docs[i] for i in top_indices if sims[i] > 0]\n",
    "    if not results:\n",
    "        # if no documents with positive similarity, fall back to top ranked regardless of positive score\n",
    "        results = [idx.docs[i] for i in top_indices]\n",
    "    return results\n",
    "\n",
    "# Search function wrapped for evaluation\n",
    "def search_fn(q):\n",
    "    query_vec = pipeline.transform([q['question']])\n",
    "    return vector_search(index, query_vec, filter_dict={'course': q['course']}, num_results=5)\n",
    "\n",
    "# MRR evaluation\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank]:\n",
    "                total_score += 1 / (rank + 1)\n",
    "                break\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "# Evaluate MRR\n",
    "relevance_total = []\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = search_fn(q)\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)\n",
    "\n",
    "score = mrr(relevance_total)\n",
    "print(\"MRR (Vector Search, question field):\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103626db",
   "metadata": {},
   "source": [
    "### Answer:3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb01273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538024e54d2f47eb8be826bda00b62e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hitrate (Vector Search question + text): 0.8210503566025502\n"
     ]
    }
   ],
   "source": [
    "# Combine question and text for document embeddings\n",
    "texts = [doc['question'] + ' ' + doc['text'] for doc in documents]\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "X = pipeline.fit_transform(texts)\n",
    "\n",
    "# Build the index\n",
    "index = minsearch.Index(text_fields=[], keyword_fields=['course', 'id'])\n",
    "index.docs = documents\n",
    "index.keyword_df = pd.DataFrame({field: [doc.get(field, '') for doc in documents] for field in index.keyword_fields})\n",
    "index.embeddings = X\n",
    "\n",
    "def vector_search(index, query_vec, filter_dict={}, num_results=5):\n",
    "    def cosine_similarity(a, b):\n",
    "        a_norm = np.linalg.norm(a)\n",
    "        b_norms = np.linalg.norm(b, axis=1)\n",
    "        return np.dot(b, a) / (b_norms * a_norm + 1e-10)\n",
    "\n",
    "    if hasattr(query_vec, 'toarray'):\n",
    "        qvec = query_vec.toarray().flatten()\n",
    "    else:\n",
    "        qvec = query_vec.flatten()\n",
    "    sims = cosine_similarity(qvec, index.embeddings)\n",
    "\n",
    "    mask = np.ones(len(index.docs), dtype=bool)\n",
    "    for field, val in filter_dict.items():\n",
    "        if field in index.keyword_fields:\n",
    "            arr = np.array(index.keyword_df[field])\n",
    "            mask = mask & (arr == val)\n",
    "\n",
    "    sims = sims * mask\n",
    "    top_indices = np.argpartition(sims, -num_results)[-num_results:]\n",
    "    top_indices = top_indices[np.argsort(-sims[top_indices])]\n",
    "    results = [index.docs[i] for i in top_indices if sims[i] > 0]\n",
    "    if not results:\n",
    "        results = [index.docs[i] for i in top_indices]\n",
    "    return results\n",
    "\n",
    "def vector_search_question_text(q):\n",
    "    query_text = q['question']\n",
    "    course_filter = q['course']\n",
    "    query_vec = pipeline.transform([query_text])\n",
    "    return vector_search(index, query_vec, filter_dict={'course': course_filter}, num_results=5)\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    return sum(True in line for line in relevance_total) / len(relevance_total)\n",
    "\n",
    "relevance_total = []\n",
    "for q in tqdm(ground_truth):\n",
    "    doc_id = q['document']\n",
    "    results = vector_search_question_text(q)\n",
    "    relevance = [d['id'] == doc_id for d in results]\n",
    "    relevance_total.append(relevance)\n",
    "\n",
    "hit = hit_rate(relevance_total)\n",
    "print(\"Hitrate (Vector Search question + text):\", hit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c755bf",
   "metadata": {},
   "source": [
    "### Answer:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf0bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import Distance\n",
    "from fastembed import TextEmbedding\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 0. Global configuration\n",
    "# ------------------------------------------------------------------#\n",
    "QDRANT_HOST      = os.getenv(\"QDRANT_HOST\", \"http://localhost:6333\")\n",
    "COLLECTION       = \"zoomcamp-rag\"\n",
    "EMBED_MODEL      = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "VECTOR_DIM       = 512\n",
    "TOP_K            = 5\n",
    "BATCH_SIZE       = 256\n",
    "TMP_CACHE_DIR    = os.path.join(tempfile.gettempdir(), \"fastembed_cache\")\n",
    "\n",
    "URL_PREFIX       = \"https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/\"\n",
    "DOCS_URL         = URL_PREFIX + \"search_evaluation/documents-with-ids.json\"\n",
    "GROUND_TRUTH_URL = URL_PREFIX + \"search_evaluation/ground-truth-data.csv\"\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 1. Metrics\n",
    "# ------------------------------------------------------------------#\n",
    "def hit_rate(relevance_total):\n",
    "    return sum(any(line) for line in relevance_total) / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total = 0.0\n",
    "    for line in relevance_total:\n",
    "        for idx, is_rel in enumerate(line):\n",
    "            if is_rel:\n",
    "                total += 1.0 / (idx + 1)\n",
    "                break\n",
    "    return total / len(relevance_total)\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 2. Download\n",
    "# ------------------------------------------------------------------#\n",
    "def download_dataset():\n",
    "    print(\"ðŸ“¥ Downloading dataset â€¦\")\n",
    "    docs   = requests.get(DOCS_URL, timeout=30).json()\n",
    "    gtruth = pd.read_csv(GROUND_TRUTH_URL).to_dict(orient=\"records\")\n",
    "    print(f\"   â†’ Documents      : {len(docs)}\")\n",
    "    print(f\"   â†’ Ground-truth Q : {len(gtruth)}\")\n",
    "    return docs, gtruth\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 3. Qdrant init\n",
    "# ------------------------------------------------------------------#\n",
    "def init_qdrant():\n",
    "    client = QdrantClient(QDRANT_HOST)\n",
    "    if client.collection_exists(COLLECTION):\n",
    "        client.delete_collection(collection_name=COLLECTION)\n",
    "        time.sleep(0.2)\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION,\n",
    "        vectors_config=models.VectorParams(size=VECTOR_DIM, distance=Distance.COSINE),\n",
    "    )\n",
    "    return client\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 4. Batch ingest with tqdm\n",
    "# ------------------------------------------------------------------#\n",
    "def ingest_documents(client, documents):\n",
    "    print(\"ðŸ§ª Loading embedding model â€¦\")\n",
    "    embedder = TextEmbedding(model_name=EMBED_MODEL, cache_dir=TMP_CACHE_DIR)\n",
    "\n",
    "    # prepare texts and ids\n",
    "    texts = [f\"{d['question']} {d['text']}\" for d in documents]\n",
    "    ids   = [d[\"id\"] for d in documents]\n",
    "\n",
    "    # embed and upsert in batches\n",
    "    for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Embedding & upsert\", unit=\"batch\"):\n",
    "        batch_texts = texts[i : i + BATCH_SIZE]\n",
    "        batch_ids   = ids[i : i + BATCH_SIZE]\n",
    "        vectors     = list(embedder.embed(batch_texts))\n",
    "        points = []\n",
    "        for pid, vec, doc in zip(batch_ids, vectors, documents[i : i + BATCH_SIZE]):\n",
    "            points.append(models.PointStruct(\n",
    "                id=pid,\n",
    "                vector=vec,\n",
    "                payload={\"question\": doc[\"question\"], \"text\": doc[\"text\"]},\n",
    "            ))\n",
    "        client.upsert(collection_name=COLLECTION, points=points)\n",
    "\n",
    "    print(\"âœ… Ingestion complete\")\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 5. Search function\n",
    "# ------------------------------------------------------------------#\n",
    "def make_search_fn(client):\n",
    "    embedder = TextEmbedding(model_name=EMBED_MODEL, cache_dir=TMP_CACHE_DIR)\n",
    "    def _search(q):\n",
    "        q_vec = next(embedder.embed([q[\"question\"]]))\n",
    "        hits  = client.search(collection_name=COLLECTION, query_vector=q_vec, limit=TOP_K)\n",
    "        return [{\"id\": hit.id} for hit in hits]\n",
    "    return _search\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 6. Evaluation\n",
    "# ------------------------------------------------------------------#\n",
    "def evaluate(ground_truth, search_fn):\n",
    "    relevance_total = []\n",
    "    for q in tqdm(ground_truth, desc=\"Evaluating\", unit=\"query\"):\n",
    "        correct_id = q[\"document\"]\n",
    "        candidates = search_fn(q)\n",
    "        relevance  = [c[\"id\"] == correct_id for c in candidates]\n",
    "        relevance_total.append(relevance)\n",
    "    return {\"hit_rate\": hit_rate(relevance_total), \"mrr\": mrr(relevance_total)}\n",
    "\n",
    "# ------------------------------------------------------------------#\n",
    "# 7. Main\n",
    "# ------------------------------------------------------------------#\n",
    "def main():\n",
    "    docs, gtruth = download_dataset()\n",
    "    client       = init_qdrant()\n",
    "    ingest_documents(client, docs)\n",
    "    search_fn    = make_search_fn(client)\n",
    "    metrics      = evaluate(gtruth, search_fn)\n",
    "\n",
    "    print(\"\\nðŸ“Š Final metrics\")\n",
    "    print(f\"Hit Rate: {metrics['hit_rate']:.4f}\")\n",
    "    print(f\"MRR      : {metrics['mrr']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff1677",
   "metadata": {},
   "source": [
    "### Answer:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48d46cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity between LLM and original answers: 0.885975885338564\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/main/03-evaluation/rag_evaluation/data/results-gpt4o-mini.csv')\n",
    "\n",
    "# Build and fit pipeline\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=3),\n",
    "    TruncatedSVD(n_components=128, random_state=1)\n",
    ")\n",
    "pipeline.fit(df['answer_llm'] + ' ' + df['answer_orig'] + ' ' + df['question'])\n",
    "\n",
    "# Prepare embeddings for llm answer + question and orig answer + question\n",
    "emb_llm = pipeline.transform(df['answer_llm'] + ' ' + df['question'])\n",
    "emb_orig = pipeline.transform(df['answer_orig'] + ' ' + df['question'])\n",
    "\n",
    "# Normalize embeddings\n",
    "emb_llm_norm = emb_llm / np.linalg.norm(emb_llm, axis=1, keepdims=True)\n",
    "emb_orig_norm = emb_orig / np.linalg.norm(emb_orig, axis=1, keepdims=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosines = np.sum(emb_llm_norm * emb_orig_norm, axis=1)\n",
    "\n",
    "# Average cosine similarity\n",
    "float(np.mean(cosines))\n",
    "print(\"Average cosine similarity between LLM and original answers:\", float(np.mean(cosines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf68549",
   "metadata": {},
   "source": [
    "### Answer:6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f26ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de139773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rouge-1 f1 score: 0.3516946452113943\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "# Load data\n",
    "path = 'https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/refs/heads/main/03-evaluation/rag_evaluation/data/results-gpt4o-mini.csv'\n",
    "df = pd.read_csv(path)\n",
    "# Initialize scorer\n",
    "rouge_scorer = Rouge()\n",
    "# Compute rouge-1 f1 for each row\n",
    "def rouge1_f1(row):\n",
    "    scores = rouge_scorer.get_scores(row['answer_llm'], row['answer_orig'])[0]\n",
    "    return scores['rouge-1']['f']\n",
    "\n",
    "# Apply and compute average\n",
    "f1_scores = df.apply(rouge1_f1, axis=1)\n",
    "avg_f1 = f1_scores.mean()\n",
    "print(\"Average rouge-1 f1 score:\", avg_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

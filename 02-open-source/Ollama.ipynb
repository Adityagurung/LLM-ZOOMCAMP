{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea3e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a9d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9f706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() \n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "#print(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506fab2a-a50c-42bd-a106-c83a9d2828ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac947de-effd-4b61-8792-a6d7a133f347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x276a77e2b10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f087272-b44d-4738-9ea2-175ec63a058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "742ab881-499a-4675-83c4-2013ea1377b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='phi3',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8bff3e-b672-42be-866b-f2d9bb217106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "988ece59-951a-4b32-ba3f-cb8efb66a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01fda569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccf92375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models (LLMs) such as GPT-3, combined with retrieval mechanisms like DPR (Dense Passage Retrieval), offer novel approaches to data engineering tasks. While traditionally involved in extracting insights and patterns from structured or semi-structured datasets using programming often require complex SQL queries for instance, integration of LLMs into these processes can simplify some aspects by understanding natural language descriptions instead which lead directly towards the task’s requirements more accurately. Here\\'selower are a few practical ways an engineer might use and benefit from this combination in their data engineering pipelines:\\n\\n1. Automated Data Preprocessing & Validation \\nNatural Language Input for Processing Job Descriptions - As LLM can understand natural language to define what should be the output, they automatically translate them into programming code where necessary or validate input job specifications written in natural languages such as “extract transactional records from March with more than $500” and convert it into SQL queries. \\n    DPR will then retrieve contextually relevant information that LLM needs to perform well on this particular task, thus reducing manual effort of translating the same sentences every time they appear in datasets or job descriptions within a pipeline step by step where data engineers describe exactly what should be done with their respective dataset. \\n    This approach may decrease human intervention and errors that tend to happen especially when dealing manually massive volume of Big Data making this technology valuable for real-time operations based on natural language interactions like chatbots in customer service systems which can now handle the data preparation part also along side other tasks (data ingestion, normalization) more efficiently. \\n2. Streamlined Feature Engineering: LLMs understand human instructions and translate them into SQL queries or code where required that help extracting vital information from primary datasets thus creating useful derived features for machine learning models used downstream in pipelines further enhancing model performance efficiency (easier to integrate these steps).\\n3. Real-Time Data Monitoring - DPR can retrieve relevant instances present within documents/tables under investigation such as logs, error messages etc that would indicate anomalies and provide engineers a chance they need with minimum search effort thereby helping catch issues fast before any consequences from those errors reach system operational or end users directly.\\n4. Dynamic Documentation Generation - By using large language models alongwith documentation generators (like Docusaurus) data engineers will now able to write descriptions for every database/API integration explaining steps required & processes in plain text format easily translated into JSON, Markdown etc which helps new developers understand how a particular step is achieved as well being helpful when trying debugging an issue related with integrating external services or APIs. This also improves collaboration where multiple engineers may need to contribute code between each stage of the pipeline while ensuring documentation maintains high quality standards (much easier and faster than manually writing them).\\n5. Code Comprehension & Documentation - Large language models can assist in reading, interpreting ,and understanding existing engineering codes such as SQL queries/python scripts where data engineers require information without diving deeper into details while seeking specific info or simply trying to confirm changes performed previously which further helps reducing mistakes and miscommunications that tend occur during code reviews. These tools also improve documentation standards since most often times we try explaining the complex logic behind certain techniques used in our own coding approach but struggle with doing this effectively & precisely because LLMs handle it much better nowadenerted by their ability to generate succinctly clear explanatory content from natural language inputs .\\n6. Improving Data Governance and Security through Natural Language Control Structures - Using large language models ,one can easily define policies(e.g.\"Only users belonging admin team should have access\" etc.) using non-technical terminology which gets translated into technical specifications needed for securing sensitive aspects without complex jargon/formulas used earlier . The implementation of AI in this respect further enhances security by automatically identifying possible risks like data breaches, and alerts can be sent to administrators accordingly taking pro-active measures thus reducing potential negative consequences faced regularly.\\n7. Optimization & Maintenance: By using LLMs we are able extract valuable insights from logs/audit trails present within infrastructure providing us ability pinpoint exact instances where resources(e.g CPU utilization, latency etc ) need optimize so engineers have better understanding along with possible recommendations of optimizations to minimize such occurrences thereby helping in long term sustainability while maintaining overall performance levels maintained as optimal or near-optimal throughout given period till required alteration . This would help identify problems earlier within system and also mitigate them quickly before resulting into major issues arising downstream steps that ultimately affect customers directly which further emphasis significance value brings forth this combination used towards data engineering.\\n8 Concluding Remark - Integrating large language models alongwith retrieval mechanisms bring lot\\'s positive results compared with conventional methods since it helps reduce manual effort , enhances accuracy, improves efficiency etc bringing various benefits helping organizations achieve their objectives quickly thus making them indispingable aspect . \\n9. Future Prospectus: This integration will further evolve as technology advances along providing real-time feedback loops & self learning capabilities which constantly fine tune performances adapt itself towards ever changing demand patterns while also creating valuable data that needs to be stored / processed thereby acting multi functional aspects thus proving their value greatly beyond simple pipeline handling making them much sought after tech component within wider industry today..'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm('How Large Language Model and Retrieval Augmented Generation can be used or useful in Data Engineering pipelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc276350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large language models (LLMs) such as GPT-3, combined with retrieval mechanisms like DPR (Dense Passage Retrieval), offer novel approaches to data engineering tasks. While traditionally involved in extracting insights and patterns from structured or semi-structured datasets using programming often require complex SQL queries for instance, integration of LLMs into these processes can simplify some aspects by understanding natural language descriptions instead which lead directly towards the task’s requirements more accurately. Here'selower are a few practical ways an engineer might use and benefit from this combination in their data engineering pipelines:\n",
      "\n",
      "1. Automated Data Preprocessing & Validation \n",
      "Natural Language Input for Processing Job Descriptions - As LLM can understand natural language to define what should be the output, they automatically translate them into programming code where necessary or validate input job specifications written in natural languages such as “extract transactional records from March with more than $500” and convert it into SQL queries. \n",
      "    DPR will then retrieve contextually relevant information that LLM needs to perform well on this particular task, thus reducing manual effort of translating the same sentences every time they appear in datasets or job descriptions within a pipeline step by step where data engineers describe exactly what should be done with their respective dataset. \n",
      "    This approach may decrease human intervention and errors that tend to happen especially when dealing manually massive volume of Big Data making this technology valuable for real-time operations based on natural language interactions like chatbots in customer service systems which can now handle the data preparation part also along side other tasks (data ingestion, normalization) more efficiently. \n",
      "2. Streamlined Feature Engineering: LLMs understand human instructions and translate them into SQL queries or code where required that help extracting vital information from primary datasets thus creating useful derived features for machine learning models used downstream in pipelines further enhancing model performance efficiency (easier to integrate these steps).\n",
      "3. Real-Time Data Monitoring - DPR can retrieve relevant instances present within documents/tables under investigation such as logs, error messages etc that would indicate anomalies and provide engineers a chance they need with minimum search effort thereby helping catch issues fast before any consequences from those errors reach system operational or end users directly.\n",
      "4. Dynamic Documentation Generation - By using large language models alongwith documentation generators (like Docusaurus) data engineers will now able to write descriptions for every database/API integration explaining steps required & processes in plain text format easily translated into JSON, Markdown etc which helps new developers understand how a particular step is achieved as well being helpful when trying debugging an issue related with integrating external services or APIs. This also improves collaboration where multiple engineers may need to contribute code between each stage of the pipeline while ensuring documentation maintains high quality standards (much easier and faster than manually writing them).\n",
      "5. Code Comprehension & Documentation - Large language models can assist in reading, interpreting ,and understanding existing engineering codes such as SQL queries/python scripts where data engineers require information without diving deeper into details while seeking specific info or simply trying to confirm changes performed previously which further helps reducing mistakes and miscommunications that tend occur during code reviews. These tools also improve documentation standards since most often times we try explaining the complex logic behind certain techniques used in our own coding approach but struggle with doing this effectively & precisely because LLMs handle it much better nowadenerted by their ability to generate succinctly clear explanatory content from natural language inputs .\n",
      "6. Improving Data Governance and Security through Natural Language Control Structures - Using large language models ,one can easily define policies(e.g.\"Only users belonging admin team should have access\" etc.) using non-technical terminology which gets translated into technical specifications needed for securing sensitive aspects without complex jargon/formulas used earlier . The implementation of AI in this respect further enhances security by automatically identifying possible risks like data breaches, and alerts can be sent to administrators accordingly taking pro-active measures thus reducing potential negative consequences faced regularly.\n",
      "7. Optimization & Maintenance: By using LLMs we are able extract valuable insights from logs/audit trails present within infrastructure providing us ability pinpoint exact instances where resources(e.g CPU utilization, latency etc ) need optimize so engineers have better understanding along with possible recommendations of optimizations to minimize such occurrences thereby helping in long term sustainability while maintaining overall performance levels maintained as optimal or near-optimal throughout given period till required alteration . This would help identify problems earlier within system and also mitigate them quickly before resulting into major issues arising downstream steps that ultimately affect customers directly which further emphasis significance value brings forth this combination used towards data engineering.\n",
      "8 Concluding Remark - Integrating large language models alongwith retrieval mechanisms bring lot's positive results compared with conventional methods since it helps reduce manual effort , enhances accuracy, improves efficiency etc bringing various benefits helping organizations achieve their objectives quickly thus making them indispingable aspect . \n",
      "9. Future Prospectus: This integration will further evolve as technology advances along providing real-time feedback loops & self learning capabilities which constantly fine tune performances adapt itself towards ever changing demand patterns while also creating valuable data that needs to be stored / processed thereby acting multi functional aspects thus proving their value greatly beyond simple pipeline handling making them much sought after tech component within wider industry today..\n"
     ]
    }
   ],
   "source": [
    "print(_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
